{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oma2ZStyqpQe"
   },
   "source": [
    "# CE-40959: Advanced Machine Learning\n",
    "## HW1 - Black-box Meta Learning (100 points)\n",
    "\n",
    "#### Name: Sepehr Ghobadi\n",
    "#### Student No: 400211008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zATL8bguriGR"
   },
   "source": [
    "In this notebook, you are going to implement a black-box meta learner using the `Omniglot` dataset.\n",
    "\n",
    "Please write your code in specified sections and do not change anything else. If you have a question regarding this homework, please ask it on the Quera.\n",
    "\n",
    "Also, it is recommended to use Google Colab to do this homework. You can connect to your drive using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pSsY1Jw7pwZc",
    "outputId": "d916513f-e19b-475d-906f-58d3936407ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EBAKrC-PkFlA",
    "outputId": "e85a2851-0fee-4593-f3bc-b23644329aa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: /content/drive/MyDrive/AdvancedML: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "results_path = '/content/drive/MyDrive/AdvancedML/HW1Q6Results/'\n",
    "%mkdir '/content/drive/MyDrive/AdvancedML/HW1Q6Results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZJ_Hv8Uqoil"
   },
   "source": [
    "## Import Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2NGBSeo0L6Vu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xabeci_XPcU2"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNHVKqRMM2oD"
   },
   "source": [
    "In Meta-Learning literature and in the meta-training phase, you are given some batches which consist of `support` and `query` sets. you train your model in a way that by using a support set you could predict query set labels correctly.\n",
    "\n",
    "In this homework, you are going to implement such meta-learner like the below architecture. In this model, at each step, you give all your support images and one query to the network simultaneously (query at the end) and you expect that the model predicts query label based on your inputs.\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<div style=\"text-align:center;\"><img src=\"https://drive.google.com/uc?export=view&id=1Au9GF7FB_IChrMLmvM0z4RBPP1R3oPgY\" width=300></div>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Don't worry if you didn't understand the architecture. we are going to explain it step by step.\n",
    "\n",
    "So if our meta-learning is K-shot N-way then each batch will consist of N*K support images with labels and one query image which we have its label in the meta-training phase.\n",
    "\n",
    "First we should build dataset it this way that each batch return N*K+1 images\n",
    "\n",
    "The Omniglot data set is designed for developing more human-like learning algorithms. It contains 1623 different handwritten characters from 50 different alphabets. Each of the 1623 characters was drawn online via Amazon's Mechanical Turk by 20 different people.\n",
    "\n",
    "Train and test dataset contains 964 and 659 classes, respectively. Torchvision-based Omniglot dataset is ordered and every 20 images in a row belong to one class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZU6nY6ZPDJla"
   },
   "outputs": [],
   "source": [
    "# Meta learning parameters.\n",
    "\n",
    "N = 5\n",
    "K = 1\n",
    "seq_length = N*K+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8bS05XnPe7v"
   },
   "source": [
    "## Prepare dataset (25 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 152,
     "referenced_widgets": [
      "86aba4cb10bc4b4bb8a7ced60826791d",
      "f128340c6ceb43f59a7485567c78ef5c",
      "463b62fee2c04785ad9524046dd648e1",
      "ea4c353a235e4ef387fb73ef48b0cd9b",
      "716d4bf01b6d427dbb951a54af153c0c",
      "e8c645eb211a4c0e9c0e4e81a4febcde",
      "bce6b128cbf94210a61a2ce632fac39f",
      "d7b005aa6d4348e0901e590b1ccf3371",
      "27910d09f178412fb1be49fdd68c04f6",
      "b787973b41614961bf704ac216647d3a",
      "6b4e5a08185b48ea81cd9a49711ccb22",
      "7f721ee2a2a1487e8a8ffc8695670035",
      "8da5fbcdd8b445bea61c7ecbc5e4b06c",
      "7af75c12ab284300aae06eb45c911477",
      "852c9cdadf73477ca2ea5cb87ef92764",
      "a445ea222bc14bb8a803bb13878039bb",
      "1a7f250512384d8e9441d8cbdb48b0ce",
      "15caa0a9c08b432aa73d2f550b770289",
      "9569d3ef863f42bd9c2843250c29bc32",
      "d47ee4a7fc66414282a14693024a2d78",
      "b23e6601eabb4cc3b2c0d8b4511b3662",
      "f9354de0246e42789e34e681625f0fdb"
     ]
    },
    "id": "pMXx6_Py9AhO",
    "outputId": "cbc71d27-af71-413a-dfff-c40827aec97a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/brendenlake/omniglot/master/python/images_background.zip to ./data/omniglot/omniglot-py/images_background.zip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86aba4cb10bc4b4bb8a7ced60826791d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9464212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/omniglot/omniglot-py/images_background.zip to ./data/omniglot/omniglot-py\n",
      "Downloading https://raw.githubusercontent.com/brendenlake/omniglot/master/python/images_evaluation.zip to ./data/omniglot/omniglot-py/images_evaluation.zip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f721ee2a2a1487e8a8ffc8695670035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6462886 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/omniglot/omniglot-py/images_evaluation.zip to ./data/omniglot/omniglot-py\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(28),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.Omniglot('./data/omniglot/', download = True, background = True, transform = transform)\n",
    "test_dataset = torchvision.datasets.Omniglot('./data/omniglot/', download = True, background = False, transform = transform)\n",
    "\n",
    "train_labels = np.repeat(np.arange(964), 20)\n",
    "test_labels = np.repeat(np.arange(659), 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWK0vksv4nVh"
   },
   "source": [
    "To build a dataloader, we should have a class that yields indexes of selected data in the dataset for every iteration and pass it to the `batch_sampler` attribute of dataloader.\n",
    "\n",
    "Complete below code based on this pseudocode:\n",
    "\n",
    "\n",
    "1.   select `N` classes randomly from all classes\n",
    "2.   select `1` class from `N` selected classes as query-contained class\n",
    "3.   select `K` images from other `N-1` classes independently and randomly\n",
    "4.   select `K+1` images from the query-contained class independently and randomly\n",
    "5.   shuffle dataset indexes, but don't forget to put query index at the last of the list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "yODgabjHEY9A"
   },
   "outputs": [],
   "source": [
    "class BatchSampler(object):\n",
    "    \"\"\"\n",
    "    BatchSampler: yield a batch of indexes at each iteration.\n",
    "    __len__ returns the number of episodes per epoch (same as 'self.iterations').\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, labels, classes_per_it, num_samples, iterations, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize the BatchSampler object\n",
    "        Arguments:\n",
    "        - labels: array of labels of dataset.\n",
    "        - classes_per_it: number of random classes for each iteration\n",
    "        - num_samples: number of samples for each iteration for each class\n",
    "        - iterations: number of iterations (episodes) per epoch\n",
    "        - batch_size: number of batches per iteration\n",
    "        \"\"\"\n",
    "        super(BatchSampler, self).__init__()\n",
    "        self.labels = labels\n",
    "        self.classes_per_it = classes_per_it\n",
    "        self.sample_per_class = num_samples\n",
    "        self.iterations = iterations\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.classes = np.unique(self.labels)\n",
    "        self.indices = np.arange(len(self.labels))\n",
    "\n",
    "    def __iter__(self):\n",
    "        '''\n",
    "        yield a batch of indexes\n",
    "        '''\n",
    "\n",
    "        for it in range(self.iterations):\n",
    "            total_batch_indexes = np.array([])\n",
    "\n",
    "            #################################################################################\n",
    "            #                  COMPLETE THE FOLLOWING SECTION (25 points)                   #\n",
    "            #################################################################################\n",
    "            # feel free to add/edit initialization part of sampler.\n",
    "            #################################################################################\n",
    "            \n",
    "            for _ in range(self.batch_size):\n",
    "                sample = np.array([], dtype=np.int64)\n",
    "                sample_classes = np.random.choice(self.classes , size=self.classes_per_it, replace=False)\n",
    "                for c in sample_classes:\n",
    "                    size = K if c!=sample_classes[-1] else K+1\n",
    "                    class_samples = np.random.choice(self.indices[self.labels==c], size=size, replace=False)\n",
    "                    sample = np.append(sample, class_samples)\n",
    "                np.random.shuffle(sample[:N*K])\n",
    "                total_batch_indexes = np.append(total_batch_indexes, sample)\n",
    "\n",
    "            #################################################################################\n",
    "            #                                   THE END                                     #\n",
    "            #################################################################################\n",
    "\n",
    "            yield total_batch_indexes.astype(int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vkaRutIUPF4b"
   },
   "outputs": [],
   "source": [
    "iterations = 5000\n",
    "batch_size = 32\n",
    "\n",
    "train_sampler = BatchSampler(labels=train_labels, classes_per_it=N,\n",
    "                              num_samples=K, iterations=iterations,\n",
    "                              batch_size=batch_size)\n",
    "\n",
    "test_sampler = BatchSampler(labels=test_labels, classes_per_it=N,\n",
    "                              num_samples=K, iterations=iterations,\n",
    "                              batch_size=batch_size)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_sampler=train_sampler)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "QCdpPyBNgDEC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnvAPmPmPh92"
   },
   "source": [
    "## Model (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52JCi9o-M2sp"
   },
   "source": [
    "Let's Build our model. the first block of our model is one encoder which is given below. you are going to implement other blocks of networks with a given explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eY3brHqVVXdb"
   },
   "outputs": [],
   "source": [
    "def conv_block(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels, momentum=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "class OmniglotNet(nn.Module):\n",
    "    '''\n",
    "    source: https://github.com/jakesnell/prototypical-networks/blob/f0c48808e496989d01db59f86d4449d7aee9ab0c/protonets/models/few_shot.py#L62-L84\n",
    "    '''\n",
    "    def __init__(self, x_dim=1, hid_dim=64, z_dim=64):\n",
    "        super(OmniglotNet, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            conv_block(x_dim, hid_dim),\n",
    "            conv_block(hid_dim, hid_dim),\n",
    "            conv_block(hid_dim, hid_dim),\n",
    "            conv_block(hid_dim, z_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vr11qLidnqqW"
   },
   "source": [
    "The whole network consists of two major blocks:\n",
    "\n",
    "\n",
    "1.   Causal Attention\n",
    "2.   Temporal Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YlOp-rImjs9d"
   },
   "source": [
    "The first block is `Causal Attention`:\n",
    "\n",
    "\n",
    "<div style=\"text-align:center;\"><img src=\"https://drive.google.com/uc?export=view&id=19lWuKzYTRry-UBog838o7dWYVL-r54WF\" width=500></div>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "The mechanism is so similar to self-attention (if you don't have any information about self-attention, see [this link](https://www.geeksforgeeks.org/self-attention-in-nlp/)) with one difference. the `masked softmax` has been replaced by `softmax`. It means that at each timestep when you calculate weights of the attention mechanism, you do it with just past keys/values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QK-n-CvSjtBV"
   },
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, key_size, value_size):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "\n",
    "        #################################################################################\n",
    "        #                  COMPLETE THE FOLLOWING SECTION (2.5 points)                  #\n",
    "        #################################################################################\n",
    "        \n",
    "        self.key_layer =  nn.Linear(in_channels, key_size)\n",
    "        self.value_layer =  nn.Linear(in_channels, value_size)\n",
    "        self.query_layer =  nn.Linear(in_channels, key_size)\n",
    "        self.temprature = np.sqrt(key_size)\n",
    "\n",
    "        #################################################################################\n",
    "        #                                   THE END                                     #\n",
    "        #################################################################################\n",
    "        self.softmax_temp = math.sqrt(key_size) #don't forget to apply temperature before calculating softmax.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is dim (N, T, in_channels) where N is the batch_size, and T is the sequence length\n",
    "        mask = np.array([[True if i>j else False for i in range(x.shape[1])] for j in range(x.shape[1])])\n",
    "        mask = torch.BoolTensor(mask).to(x.device)\n",
    "\n",
    "      \n",
    "        #################################################################################\n",
    "        #                  COMPLETE THE FOLLOWING SECTION (7.5 points)                  #\n",
    "        #################################################################################\n",
    "    \n",
    "        weights = torch.matmul( self.query_layer(x), torch.transpose(self.key_layer(x), 2, 1) )\n",
    "        weights.data[:,mask] = -float('inf')\n",
    "        weights = F.softmax(weights/self.temprature, dim=1)\n",
    "        attention = torch.matmul(weights, self.value_layer(x))\n",
    "        return torch.cat([attention, x], dim=2)\n",
    "\n",
    "        #################################################################################\n",
    "        #                                   THE END                                     #\n",
    "        #################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8V3lTcd30tW"
   },
   "source": [
    "The second block is `Temporal Convolution`:\n",
    "\n",
    "a Temporal Convolution consists of a series of `Dense Blocks` whose dilation rates increase exponentially until their receptive field exceeds the desired sequence length. For example first time when you apply this block, sequence length is (N*K+1) and dilation is 2.\n",
    "to sum up, what you will do is this:\n",
    "\n",
    "<div style=\"text-align:center;\"><img src=\"https://drive.google.com/uc?export=view&id=1_mWTFiZNQlN4sMTWp2GqolSSzNTAFJuh\" width=1000></div>\n",
    "\n",
    "<br>\n",
    "Dense Block pseduocode is:\n",
    "<br><br>\n",
    "\n",
    "<div style=\"text-align:center;\"><img src=\"https://drive.google.com/uc?export=view&id=1T2q6KugqBEcwSyJAAGymTaXTe__MGsv3\" width=1000></div>\n",
    "\n",
    "<br>\n",
    "The `CausalConv` code is given.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "G2wn9BPeUHPY"
   },
   "outputs": [],
   "source": [
    "class CasualConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dilation=1):\n",
    "        super(CasualConv1d, self).__init__()\n",
    "\n",
    "        self.pad = nn.ConstantPad1d((dilation, 0), 0)\n",
    "        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size=2, dilation=dilation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv1d(self.pad(x))\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, dilation):\n",
    "        super().__init__()\n",
    "\n",
    "        #################################################################################\n",
    "        #                  COMPLETE THE FOLLOWING SECTION (2.5 points)                  #\n",
    "        #################################################################################\n",
    "        \n",
    "        self.casualconvf = CasualConv1d(in_channels, out_channels, dilation=dilation)\n",
    "        self.casualconvg = CasualConv1d(in_channels, out_channels, dilation=dilation)\n",
    "\n",
    "        #################################################################################\n",
    "        #                                   THE END                                     #\n",
    "        #################################################################################\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        #################################################################################\n",
    "        #                  COMPLETE THE FOLLOWING SECTION (5 points)                    #\n",
    "        #################################################################################\n",
    "\n",
    "        xf, xg = self.casualconvf(x), self.casualconvg(x)\n",
    "        return torch.cat((x, torch.tanh(xf) * torch.sigmoid(xg)), dim=1) #functional.tanh is deprecated\n",
    "    \n",
    "        #################################################################################\n",
    "        #                                   THE END                                     #\n",
    "        #################################################################################\n",
    "\n",
    "\n",
    "\n",
    "class TemporalConvolutionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, sequence_length, in_channels, dense_block_out_channels, levels):\n",
    "        super().__init__()\n",
    "\n",
    "        #################################################################################\n",
    "        #                  COMPLETE THE FOLLOWING SECTION (2.5 points)                  #\n",
    "        #################################################################################\n",
    "        \n",
    "        #dense_blocks = [DenseBlock(in_channels, dense_block_out_channels, 2)] + [DenseBlock(dense_block_out_channels, dense_block_out_channels, 2**(l+1)) for l in range(1,levels)]\n",
    "        dense_blocks = [DenseBlock(in_channels+l*dense_block_out_channels, dense_block_out_channels, 2**(l+1)) for l in range(levels)]\n",
    "        self.dense_blocks = nn.ModuleList(dense_blocks)\n",
    "        \n",
    "        #################################################################################\n",
    "        #                                   THE END                                     #\n",
    "        #################################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        #################################################################################\n",
    "        #                  COMPLETE THE FOLLOWING SECTION (10 points)                   #\n",
    "        #################################################################################\n",
    "        x = torch.transpose(x, 2, 1)\n",
    "        for block in self.dense_blocks:\n",
    "            x = block(x)\n",
    "        x = torch.transpose(x, 2, 1)\n",
    "        return x\n",
    "    \n",
    "        #################################################################################\n",
    "        #                                   THE END                                     #\n",
    "        #################################################################################\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nZjfogMlnApy"
   },
   "outputs": [],
   "source": [
    "# The general mechanism of the network is as follows:\n",
    "# Your input shape is (B*S, C, H, W). B is batch size, S is sequence length, C is channel, H is height and W is width of your image.\n",
    "# first you should pass your input to \"OmniglotNet\" network to get feature vectors per data. shape: (B*S, V). V is feature vector size.\n",
    "# then separate B and S dimensions and concat one-hot labels with your data. Shape: (B, S, V + N). N is your meta-learner parameter (number of classes per batch)\n",
    "# pass it to a attention block with key size of 64 and value size of 32. shape: (B, S, v1)\n",
    "# pass it to a temporal convolution block which consists of dense blocks with 128 output channels. shape: (B, S, v2)\n",
    "# pass it to a attention block with key size of 256 and value size of 128. shape: (B, S, v3)\n",
    "# pass it to a temporal convolution block which consists of dense blocks with 128 output channels. shape: (B, S, v4)\n",
    "# pass it to a attention block with key size of 512 and value size of 256. shape: (B, S, v5)\n",
    "# pass it to a Linear block with N outputs to predict labels. shape: (B, S, N)\n",
    "# return last index of sequence which is related to query (second dimension). shape: (B, N)\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, N, K):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.N = N\n",
    "        self.K = K\n",
    "        self.encoder = OmniglotNet().double()\n",
    "        channels_number = 64 + N\n",
    "        seq_legth = N*K+1\n",
    "        dense_block_out_channels = 128\n",
    "        dense_block_levels = int(np.ceil(np.log2(seq_legth)))\n",
    "\n",
    "        #################################################################################\n",
    "        #                  COMPLETE THE FOLLOWING SECTION (10 points)                   #\n",
    "        #################################################################################\n",
    "\n",
    "\n",
    "        key_size, value_size = 64, 32\n",
    "        self.attention1 = AttentionBlock(in_channels=channels_number, key_size=key_size, value_size=value_size)\n",
    "        channels_number += value_size\n",
    "        \n",
    "        self.tempconv1 = TemporalConvolutionBlock(seq_legth, in_channels=channels_number, dense_block_out_channels=dense_block_out_channels, levels=dense_block_levels)\n",
    "        channels_number += dense_block_levels*dense_block_out_channels\n",
    "        \n",
    "        key_size, value_size = 256, 128\n",
    "        self.attention2 = AttentionBlock(in_channels=channels_number, key_size=key_size, value_size=value_size)\n",
    "        channels_number += value_size\n",
    "        \n",
    "        self.tempconv2 = TemporalConvolutionBlock(seq_legth, in_channels=channels_number, dense_block_out_channels = 128, levels=dense_block_levels)\n",
    "        channels_number += dense_block_levels*dense_block_out_channels\n",
    "        \n",
    "        key_size, value_size = 512, 256\n",
    "        self.attention3 = AttentionBlock(in_channels=channels_number, key_size=key_size, value_size=value_size)\n",
    "        channels_number += value_size\n",
    "        \n",
    "        self.fc = nn.Linear(channels_number, N)\n",
    "\n",
    "\n",
    "        #################################################################################\n",
    "        #                                   THE END                                     #\n",
    "        #################################################################################\n",
    "\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "\n",
    "        #################################################################################\n",
    "        #                  COMPLETE THE FOLLOWING SECTION (10 points)                   #\n",
    "        #################################################################################\n",
    "        # input shape is (B*S, C, H, W)\n",
    "        # labels shape is (B, S, N)\n",
    "        # output shape is (N, N)\n",
    "        # calculate output by given description\n",
    "        #################################################################################\n",
    "\n",
    "        output = self.encoder(input)\n",
    "        output = torch.cat((output, labels), dim=1)\n",
    "        output = output.view((batch_size, self.N*self.K+1, -1))\n",
    "        output = self.attention1(output)\n",
    "        output = self.tempconv1(output)\n",
    "        output = self.attention2(output)\n",
    "        output = self.tempconv2(output)\n",
    "        output = self.attention3(output)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return torch.squeeze(output[:,-1:])\n",
    "        #################################################################################\n",
    "        #                                   THE END                                     #\n",
    "        #################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLQCj-HCgDEF"
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Lv2PFeyKgDEG"
   },
   "outputs": [],
   "source": [
    "def get_onehot_labels(labels, seq_length, batch_size, device):\n",
    "    one_hots = []\n",
    "    for s in range(batch_size):\n",
    "        sample = labels[s*seq_length:(s+1)*seq_length]\n",
    "        indices = [ np.where(sorted(sample[:-1].cpu().numpy()) == c)[0][0] for c in sample.cpu().numpy()]\n",
    "        one_hot = np.zeros((seq_length, N))\n",
    "        one_hot[np.arange(seq_length), indices] = 1\n",
    "        one_hots.append(torch.tensor(one_hot))\n",
    "\n",
    "    return torch.cat(one_hots, dim=0).to(device)\n",
    "\n",
    "def mask_labels(one_hot_labels, seq_length, batch_size, device):\n",
    "    query_indices = [ s*seq_length-1 for s in range(1,batch_size+1) ]\n",
    "    masked_labels = one_hot_labels.detach().clone()\n",
    "    masked_labels[ query_indices ] = torch.tensor(np.zeros((batch_size, N)), dtype=torch.float64, device=device) # we have #batch_size queries\n",
    "    return masked_labels, query_indices\n",
    "\n",
    "def eval_model(logits, targets, criterion):\n",
    "    loss = criterion(logits, targets)\n",
    "    _, output_labels = logits.max(dim=1)\n",
    "    _, labels = targets.max(dim=1)\n",
    "    acc = torch.eq(labels, output_labels).double().mean().item()\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "si_-sVXgbYbr"
   },
   "source": [
    "## Train (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vwyVINHPbc5q",
    "outputId": "7ba45036-7bee-4fe7-9a4c-7bd07e451495",
    "scrolled": true
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [13:37<00:00,  6.12it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3241444389015309  ---------  Train Accuracy: 0.40656875\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [12:24<00:00,  6.71it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9122364922539528  ---------  Validation Accuracy: 0.65801875\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [13:37<00:00,  6.12it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.19941467473051244  ---------  Train Accuracy: 0.9278625\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [12:20<00:00,  6.75it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3070856669964922  ---------  Validation Accuracy: 0.8918\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [13:35<00:00,  6.13it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0922963682489667  ---------  Train Accuracy: 0.96695\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [11:47<00:00,  7.06it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.24903771602297883  ---------  Validation Accuracy: 0.9158375\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [12:28<00:00,  6.68it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.06598298080398533  ---------  Train Accuracy: 0.97685625\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [11:47<00:00,  7.06it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2263014063565537  ---------  Validation Accuracy: 0.92541875\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [12:49<00:00,  6.50it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.05312101418651987  ---------  Train Accuracy: 0.9811125\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [11:52<00:00,  7.02it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.21911597323845236  ---------  Validation Accuracy: 0.92783125\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [12:51<00:00,  6.48it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.04415627173569222  ---------  Train Accuracy: 0.984325\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [11:59<00:00,  6.94it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1777102025687355  ---------  Validation Accuracy: 0.941375\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [13:25<00:00,  6.20it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.03847476034950947  ---------  Train Accuracy: 0.98635\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [11:54<00:00,  7.00it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1627938081292993  ---------  Validation Accuracy: 0.9460375\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [12:53<00:00,  6.46it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.033860022948349174  ---------  Train Accuracy: 0.98775625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [11:55<00:00,  6.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.15936860225827423  ---------  Validation Accuracy: 0.94793125\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 81/5000 [00:13<13:37,  6.01it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-347a774489d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-74073029007f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, labels)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtempconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtempconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-8822f2d85d3f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-8822f2d85d3f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m#################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mxf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcasualconvf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcasualconvg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#functional.tanh is deprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-8822f2d85d3f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDenseBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/padding.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'constant'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_pad\u001b[0;34m(input, pad, mode, value)\u001b[0m\n\u001b[1;32m   4172\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Padding length too large\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4173\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"constant\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_pad_nd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4175\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4176\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Padding mode \"{}\"\" doesn\\'t take in value argument'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "lr = 0.0001\n",
    "model = Network(N,K)\n",
    "model = model.double()\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr)\n",
    "epochs = 30\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loss, train_acc, best_acc = [], [], 0.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch {}/{}'.format(epoch+1, epochs))\n",
    "    model.train()\n",
    "    loader_iter = iter(train_dataloader)\n",
    "    epoch_loss, epoch_acc = [], []\n",
    "    for inputs, labels in tqdm(loader_iter):\n",
    "        #################################################################################\n",
    "        #                  COMPLETE THE FOLLOWING SECTION (15 points)                   #\n",
    "        #################################################################################\n",
    "        # prepare your data as input to your model.\n",
    "        # extract query label (last image label in each batch) for loss function.\n",
    "        # convert your labels to one-hot form and don't forget to set all elements of\n",
    "        # one-hotted query label to zero (it's trivial that we shouldn't give\n",
    "        # the output of the network to model as input!).\n",
    "        # train your model.\n",
    "        # save loss of each iteration\n",
    "        #################################################################################\n",
    "        inputs = inputs.double().to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # creating one hot labels\n",
    "        one_hot_labels = get_onehot_labels(labels, seq_length, batch_size, device)\n",
    "        \n",
    "        #masking query labels to all zero\n",
    "        masked_labels, query_indices = mask_labels(one_hot_labels, seq_length, batch_size, device)\n",
    "        \n",
    "        \n",
    "        logits = model(inputs, masked_labels)\n",
    "        loss, acc = eval_model(logits, one_hot_labels[query_indices], criterion)\n",
    "        \n",
    "        epoch_loss.append(loss.item())\n",
    "        epoch_acc.append(acc)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_loss = np.mean(epoch_loss)\n",
    "    train_acc = np.mean(epoch_acc)\n",
    "    \n",
    "    model_state = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "        }\n",
    "    torch.save(model_state, results_path+'model_train_state.pt')\n",
    "\n",
    "    print(f\"Train Loss: {train_loss}  ---------  Train Accuracy: {train_acc}\")\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    loader_iter = iter(test_dataloader)\n",
    "    epoch_loss, epoch_acc = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(loader_iter):\n",
    "            #################################################################################\n",
    "            #                  COMPLETE THE FOLLOWING SECTION (5 points)                    #\n",
    "            #################################################################################\n",
    "            # report accuracy of your model.\n",
    "            # plot loss values in whole training iterations.\n",
    "            #################################################################################\n",
    "            inputs = inputs.double().to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # creating one hot labels\n",
    "            one_hot_labels = get_onehot_labels(labels, seq_length, batch_size, device)\n",
    "\n",
    "            #masking query labels to all zero\n",
    "            masked_labels, query_indices = mask_labels(one_hot_labels, seq_length, batch_size, device)\n",
    "\n",
    "            logits = model(inputs, masked_labels)\n",
    "            loss, acc = eval_model(logits, one_hot_labels[query_indices], criterion)\n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "            epoch_acc.append(acc)\n",
    "\n",
    "    val_loss = np.mean(epoch_loss)\n",
    "    val_acc = np.mean(epoch_acc)\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss}  ---------  Validation Accuracy: {val_acc}\")\n",
    "\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        torch.save(model_state, results_path+'best_model_state.pt')\n",
    "        best_acc = val_acc\n",
    "    epoch_result = {\n",
    "        'epoch': epoch,\n",
    "        'train_loss': train_loss,\n",
    "        'train_acc': train_acc,\n",
    "        'val_loss': val_loss,\n",
    "        'val_acc': val_acc,\n",
    "        'model': model_state\n",
    "    }\n",
    "    pickle.dump(epoch_result, open(os.path.join(results_path, f\"train_epoch_{epoch}.p\"), 'wb'))\n",
    "\n",
    "        #################################################################################\n",
    "        #                                   THE END                                     #\n",
    "        #################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSV1d5zFPuH6"
   },
   "source": [
    "## Test (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ALu1EEDQ0RQ",
    "outputId": "61ca7698-99b4-4cf9-9cb2-e921c2aafb40"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [12:36<00:00,  6.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.9471875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "lr = 0.0001\n",
    "model = Network(N,K)\n",
    "model = model.double()\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr)\n",
    "epochs = 30\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_model_path = results_path+'best_model_state.pt'\n",
    "state_dict = torch.load(best_model_path, map_location=device)\n",
    "model.load_state_dict(state_dict['model_state'], strict=False)\n",
    "optimizer.load_state_dict(state_dict['optimizer_state'])\n",
    "\n",
    "test_epochs = 1\n",
    "for epoch in range(test_epochs):\n",
    "    loader_iter = iter(test_dataloader)\n",
    "    epoch_loss, epoch_acc = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(loader_iter):\n",
    "            #################################################################################\n",
    "            #                  COMPLETE THE FOLLOWING SECTION (5 points)                    #\n",
    "            #################################################################################\n",
    "            # report accuracy of your model.\n",
    "            # plot loss values in whole training iterations.\n",
    "            #################################################################################\n",
    "            inputs = inputs.double().to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # creating one hot labels\n",
    "            one_hot_labels = get_onehot_labels(labels, seq_length, batch_size, device)\n",
    "\n",
    "            #masking query labels to all zero\n",
    "            masked_labels, query_indices = mask_labels(one_hot_labels, seq_length, batch_size, device)\n",
    "\n",
    "            logits = model(inputs, masked_labels)\n",
    "            loss, acc = eval_model(logits, one_hot_labels[query_indices], criterion)\n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "            epoch_acc.append(acc)\n",
    "    \n",
    "    print(f\"\\nTest Accuracy: {np.mean(epoch_acc)}\")\n",
    "\n",
    "\n",
    "\n",
    "        #################################################################################\n",
    "        #                                   THE END                                     #\n",
    "        #################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AfXPU6FQAwX"
   },
   "source": [
    "## Question (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Za9ptSATXq2"
   },
   "source": [
    "Question) State one problem of using this network for meta-learning\n",
    "<br><br>\n",
    "\n",
    "Answer: one problem is that  few-shot learning is not inherently a sequential problem and using temporal convolutions imposes a causality hypothesis on dataset which is not a good assumption for supervised classification. also in RL this window of context causallity could not be determined easily. in total using reurrent based archittectures for classification problems create an order assumption on dataset which s assumed to be i.i.d. . also due to the design of this architecture model has high number of parameters which can lead to overfitting on extremely out-of-distribution tasks.  these out of distribution tasks can not be tested on datasets like omniglot or even imagenet. also this type of meta-learners which are black box style on completely out of distribution may not converge to optimum solution even ini long runs oppose to optimization-based meta learning algorithms.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k6UiTrdkstq9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Black_box_Meta_Learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "15caa0a9c08b432aa73d2f550b770289": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1a7f250512384d8e9441d8cbdb48b0ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "27910d09f178412fb1be49fdd68c04f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "463b62fee2c04785ad9524046dd648e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d7b005aa6d4348e0901e590b1ccf3371",
      "max": 9464212,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_27910d09f178412fb1be49fdd68c04f6",
      "value": 9464212
     }
    },
    "6b4e5a08185b48ea81cd9a49711ccb22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "716d4bf01b6d427dbb951a54af153c0c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7af75c12ab284300aae06eb45c911477": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9569d3ef863f42bd9c2843250c29bc32",
      "max": 6462886,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d47ee4a7fc66414282a14693024a2d78",
      "value": 6462886
     }
    },
    "7f721ee2a2a1487e8a8ffc8695670035": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8da5fbcdd8b445bea61c7ecbc5e4b06c",
       "IPY_MODEL_7af75c12ab284300aae06eb45c911477",
       "IPY_MODEL_852c9cdadf73477ca2ea5cb87ef92764"
      ],
      "layout": "IPY_MODEL_a445ea222bc14bb8a803bb13878039bb"
     }
    },
    "852c9cdadf73477ca2ea5cb87ef92764": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b23e6601eabb4cc3b2c0d8b4511b3662",
      "placeholder": "​",
      "style": "IPY_MODEL_f9354de0246e42789e34e681625f0fdb",
      "value": " 6463488/? [00:00&lt;00:00, 11897132.72it/s]"
     }
    },
    "86aba4cb10bc4b4bb8a7ced60826791d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f128340c6ceb43f59a7485567c78ef5c",
       "IPY_MODEL_463b62fee2c04785ad9524046dd648e1",
       "IPY_MODEL_ea4c353a235e4ef387fb73ef48b0cd9b"
      ],
      "layout": "IPY_MODEL_716d4bf01b6d427dbb951a54af153c0c"
     }
    },
    "8da5fbcdd8b445bea61c7ecbc5e4b06c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1a7f250512384d8e9441d8cbdb48b0ce",
      "placeholder": "​",
      "style": "IPY_MODEL_15caa0a9c08b432aa73d2f550b770289",
      "value": ""
     }
    },
    "9569d3ef863f42bd9c2843250c29bc32": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a445ea222bc14bb8a803bb13878039bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b23e6601eabb4cc3b2c0d8b4511b3662": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b787973b41614961bf704ac216647d3a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bce6b128cbf94210a61a2ce632fac39f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d47ee4a7fc66414282a14693024a2d78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d7b005aa6d4348e0901e590b1ccf3371": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8c645eb211a4c0e9c0e4e81a4febcde": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea4c353a235e4ef387fb73ef48b0cd9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b787973b41614961bf704ac216647d3a",
      "placeholder": "​",
      "style": "IPY_MODEL_6b4e5a08185b48ea81cd9a49711ccb22",
      "value": " 9464832/? [00:00&lt;00:00, 47459932.53it/s]"
     }
    },
    "f128340c6ceb43f59a7485567c78ef5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e8c645eb211a4c0e9c0e4e81a4febcde",
      "placeholder": "​",
      "style": "IPY_MODEL_bce6b128cbf94210a61a2ce632fac39f",
      "value": ""
     }
    },
    "f9354de0246e42789e34e681625f0fdb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
